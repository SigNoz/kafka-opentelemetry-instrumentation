ResourceMetrics #0
Resource SchemaURL:
Resource attributes:
     -> service.name: Str(unknown_service:java)
     -> telemetry.sdk.language: Str(java)
     -> telemetry.sdk.name: Str(opentelemetry)
     -> telemetry.sdk.version: Str(1.35.0)
ScopeMetrics #0
ScopeMetrics SchemaURL:
InstrumentationScope io.opentelemetry.contrib.jmxmetrics 1.33.0-alpha
Metric #0
Descriptor:
     -> Name: kafka.leader.election.rate
     -> Description: leader election rate - increasing indicates broker failures
     -> Unit: {elections}
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0
Metric #1
Descriptor:
     -> Name: kafka.request.failed
     -> Description: The number of requests to the broker resulting in a failure
     -> Unit: {requests}
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
Data point attributes:
     -> type: Str(fetch)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0
NumberDataPoints #1
Data point attributes:
     -> type: Str(produce)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0
Metric #2
Descriptor:
     -> Name: kafka.purgatory.size
     -> Description: The number of requests waiting in purgatory
     -> Unit: {requests}
     -> DataType: Gauge
NumberDataPoints #0
Data point attributes:
     -> type: Str(fetch)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0
NumberDataPoints #1
Data point attributes:
     -> type: Str(produce)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0
Metric #3
Descriptor:
     -> Name: kafka.request.time.avg
     -> Description: The average time the broker has taken to service requests
     -> Unit: ms
     -> DataType: Gauge
NumberDataPoints #0
Data point attributes:
     -> type: Str(fetchfollower)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0.000000
NumberDataPoints #1
Data point attributes:
     -> type: Str(produce)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0.000000
NumberDataPoints #2
Data point attributes:
     -> type: Str(fetchconsumer)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0.000000
Metric #4
Descriptor:
     -> Name: kafka.controller.active.count
     -> Description: controller is active on broker
     -> Unit: {controllers}
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 1
Metric #5
Descriptor:
     -> Name: kafka.request.time.total
     -> Description: The total time the broker has taken to service requests
     -> Unit: ms
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
Data point attributes:
     -> type: Str(fetchfollower)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0
NumberDataPoints #1
Data point attributes:
     -> type: Str(produce)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0
NumberDataPoints #2
Data point attributes:
     -> type: Str(fetchconsumer)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0
Metric #6
Descriptor:
     -> Name: kafka.logs.flush.time.99p
     -> Description: log flush time - 99th percentile
     -> Unit: ms
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0.000000
Metric #7
Descriptor:
     -> Name: kafka.partition.under_replicated
     -> Description: The number of under replicated partitions
     -> Unit: {partitions}
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0
Metric #8
Descriptor:
     -> Name: kafka.logs.flush.time.median
     -> Description: log flush time - 50th percentile
     -> Unit: ms
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0.000000
Metric #9
Descriptor:
     -> Name: kafka.unclean.election.rate
     -> Description: unclean leader election rate - increasing indicates broker failures
     -> Unit: {elections}
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0
Metric #10
Descriptor:
     -> Name: kafka.network.io
     -> Description: The bytes received or sent by the broker
     -> Unit: by
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
Data point attributes:
     -> state: Str(out)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0
NumberDataPoints #1
Data point attributes:
     -> state: Str(in)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0
Metric #11
Descriptor:
     -> Name: kafka.partition.offline
     -> Description: The number of partitions offline
     -> Unit: {partitions}
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 1
Metric #12
Descriptor:
     -> Name: kafka.request.time.50p
     -> Description: The 50th percentile time the broker has taken to service requests
     -> Unit: ms
     -> DataType: Gauge
NumberDataPoints #0
Data point attributes:
     -> type: Str(fetchfollower)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0.000000
NumberDataPoints #1
Data point attributes:
     -> type: Str(produce)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0.000000
NumberDataPoints #2
Data point attributes:
     -> type: Str(fetchconsumer)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0.000000
Metric #13
Descriptor:
     -> Name: kafka.logs.flush.time.count
     -> Description: log flush count
     -> Unit: ms
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0
Metric #14
Descriptor:
     -> Name: kafka.isr.operation.count
     -> Description: The number of in-sync replica shrink and expand operations
     -> Unit: {operations}
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
Data point attributes:
     -> operation: Str(shrink)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0
NumberDataPoints #1
Data point attributes:
     -> operation: Str(expand)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0
Metric #15
Descriptor:
     -> Name: kafka.request.queue
     -> Description: size of the request queue
     -> Unit: {requests}
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0
Metric #16
Descriptor:
     -> Name: kafka.request.time.99p
     -> Description: The 99th percentile time the broker has taken to service requests
     -> Unit: ms
     -> DataType: Gauge
NumberDataPoints #0
Data point attributes:
     -> type: Str(fetchfollower)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0.000000
NumberDataPoints #1
Data point attributes:
     -> type: Str(produce)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0.000000
NumberDataPoints #2
Data point attributes:
     -> type: Str(fetchconsumer)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0.000000
Metric #17
Descriptor:
     -> Name: kafka.request.count
     -> Description: The number of requests received by the broker
     -> Unit: {requests}
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
Data point attributes:
     -> type: Str(fetch)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0
NumberDataPoints #1
Data point attributes:
     -> type: Str(produce)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0
Metric #18
Descriptor:
     -> Name: kafka.message.count
     -> Description: The number of messages received by the broker
     -> Unit: {messages}
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0
Metric #19
Descriptor:
     -> Name: kafka.partition.count
     -> Description: The number of partitions on the broker
     -> Unit: {partitions}
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 1
Metric #20
Descriptor:
     -> Name: kafka.max.lag
     -> Description: max lag in messages between follower and leader replicas
     -> Unit: {messages}
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0
ScopeMetrics #1
ScopeMetrics SchemaURL:
InstrumentationScope io.opentelemetry.sdk.logs
Metric #0
Descriptor:
     -> Name: queueSize
     -> Description: The number of items queued
     -> Unit: 1
     -> DataType: Gauge
NumberDataPoints #0
Data point attributes:
     -> processorType: Str(BatchLogRecordProcessor)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:02.902 +0000 UTC
Value: 0
	{"kind": "exporter", "data_type": "metrics", "name": "debug"}
2024-05-22T17:58:12.936+0200	info	MetricsExporter	{"kind": "exporter", "data_type": "metrics", "name": "debug", "resource metrics": 1, "metrics": 22, "data points": 35}
2024-05-22T17:58:12.936+0200	info	ResourceMetrics #0
Resource SchemaURL:
Resource attributes:
     -> service.name: Str(unknown_service:java)
     -> telemetry.sdk.language: Str(java)
     -> telemetry.sdk.name: Str(opentelemetry)
     -> telemetry.sdk.version: Str(1.35.0)
ScopeMetrics #0
ScopeMetrics SchemaURL:
InstrumentationScope io.opentelemetry.contrib.jmxmetrics 1.33.0-alpha
Metric #0
Descriptor:
     -> Name: kafka.leader.election.rate
     -> Description: leader election rate - increasing indicates broker failures
     -> Unit: {elections}
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0
Metric #1
Descriptor:
     -> Name: kafka.request.failed
     -> Description: The number of requests to the broker resulting in a failure
     -> Unit: {requests}
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
Data point attributes:
     -> type: Str(fetch)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0
NumberDataPoints #1
Data point attributes:
     -> type: Str(produce)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0
Metric #2
Descriptor:
     -> Name: kafka.purgatory.size
     -> Description: The number of requests waiting in purgatory
     -> Unit: {requests}
     -> DataType: Gauge
NumberDataPoints #0
Data point attributes:
     -> type: Str(fetch)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0
NumberDataPoints #1
Data point attributes:
     -> type: Str(produce)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0
Metric #3
Descriptor:
     -> Name: kafka.request.time.avg
     -> Description: The average time the broker has taken to service requests
     -> Unit: ms
     -> DataType: Gauge
NumberDataPoints #0
Data point attributes:
     -> type: Str(fetchfollower)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0.000000
NumberDataPoints #1
Data point attributes:
     -> type: Str(produce)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 43.000000
NumberDataPoints #2
Data point attributes:
     -> type: Str(fetchconsumer)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0.000000
Metric #4
Descriptor:
     -> Name: kafka.controller.active.count
     -> Description: controller is active on broker
     -> Unit: {controllers}
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 1
Metric #5
Descriptor:
     -> Name: kafka.request.time.total
     -> Description: The total time the broker has taken to service requests
     -> Unit: ms
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
Data point attributes:
     -> type: Str(fetchfollower)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0
NumberDataPoints #1
Data point attributes:
     -> type: Str(produce)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 1
NumberDataPoints #2
Data point attributes:
     -> type: Str(fetchconsumer)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0
Metric #6
Descriptor:
     -> Name: kafka.logs.flush.time.99p
     -> Description: log flush time - 99th percentile
     -> Unit: ms
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0.000000
Metric #7
Descriptor:
     -> Name: kafka.partition.under_replicated
     -> Description: The number of under replicated partitions
     -> Unit: {partitions}
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0
Metric #8
Descriptor:
     -> Name: kafka.logs.flush.time.median
     -> Description: log flush time - 50th percentile
     -> Unit: ms
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0.000000
Metric #9
Descriptor:
     -> Name: kafka.unclean.election.rate
     -> Description: unclean leader election rate - increasing indicates broker failures
     -> Unit: {elections}
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0
Metric #10
Descriptor:
     -> Name: kafka.network.io
     -> Description: The bytes received or sent by the broker
     -> Unit: by
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
Data point attributes:
     -> state: Str(out)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0
NumberDataPoints #1
Data point attributes:
     -> state: Str(in)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 4751
Metric #11
Descriptor:
     -> Name: kafka.partition.offline
     -> Description: The number of partitions offline
     -> Unit: {partitions}
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 1
Metric #12
Descriptor:
     -> Name: kafka.request.time.50p
     -> Description: The 50th percentile time the broker has taken to service requests
     -> Unit: ms
     -> DataType: Gauge
NumberDataPoints #0
Data point attributes:
     -> type: Str(fetchfollower)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0.000000
NumberDataPoints #1
Data point attributes:
     -> type: Str(produce)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 43.000000
NumberDataPoints #2
Data point attributes:
     -> type: Str(fetchconsumer)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0.000000
Metric #13
Descriptor:
     -> Name: kafka.logs.flush.time.count
     -> Description: log flush count
     -> Unit: ms
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0
Metric #14
Descriptor:
     -> Name: kafka.isr.operation.count
     -> Description: The number of in-sync replica shrink and expand operations
     -> Unit: {operations}
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
Data point attributes:
     -> operation: Str(shrink)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0
NumberDataPoints #1
Data point attributes:
     -> operation: Str(expand)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0
Metric #15
Descriptor:
     -> Name: kafka.request.queue
     -> Description: size of the request queue
     -> Unit: {requests}
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0
Metric #16
Descriptor:
     -> Name: kafka.request.time.99p
     -> Description: The 99th percentile time the broker has taken to service requests
     -> Unit: ms
     -> DataType: Gauge
NumberDataPoints #0
Data point attributes:
     -> type: Str(fetchfollower)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0.000000
NumberDataPoints #1
Data point attributes:
     -> type: Str(produce)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 43.000000
NumberDataPoints #2
Data point attributes:
     -> type: Str(fetchconsumer)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0.000000
Metric #17
Descriptor:
     -> Name: kafka.request.count
     -> Description: The number of requests received by the broker
     -> Unit: {requests}
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
Data point attributes:
     -> type: Str(fetch)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0
NumberDataPoints #1
Data point attributes:
     -> type: Str(produce)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 1
Metric #18
Descriptor:
     -> Name: kafka.message.count
     -> Description: The number of messages received by the broker
     -> Unit: {messages}
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 50
Metric #19
Descriptor:
     -> Name: kafka.partition.count
     -> Description: The number of partitions on the broker
     -> Unit: {partitions}
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 1
Metric #20
Descriptor:
     -> Name: kafka.max.lag
     -> Description: max lag in messages between follower and leader replicas
     -> Unit: {messages}
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0
ScopeMetrics #1
ScopeMetrics SchemaURL:
InstrumentationScope io.opentelemetry.sdk.logs
Metric #0
Descriptor:
     -> Name: queueSize
     -> Description: The number of items queued
     -> Unit: 1
     -> DataType: Gauge
NumberDataPoints #0
Data point attributes:
     -> processorType: Str(BatchLogRecordProcessor)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:12.898 +0000 UTC
Value: 0
	{"kind": "exporter", "data_type": "metrics", "name": "debug"}
2024-05-22T17:58:15.818+0200	info	MetricsExporter	{"kind": "exporter", "data_type": "metrics", "name": "debug", "resource metrics": 1, "metrics": 7, "data points": 10}
2024-05-22T17:58:15.818+0200	info	ResourceMetrics #0
Resource SchemaURL: https://opentelemetry.io/schemas/1.24.0
Resource attributes:
     -> host.arch: Str(aarch64)
     -> host.name: Str(xs-MacBook-Pro.local)
     -> os.description: Str(Mac OS X 14.1.1)
     -> os.type: Str(darwin)
     -> process.command_args: Slice(["/Users/x/Library/Java/JavaVirtualMachines/openjdk-22.0.1/Contents/Home/bin/java","-javaagent:/Users/x/oss/kafka-opentelemetry-instrumentation/opentelemetry-javagent/opentelemetry-javaagent.jar","-Dotel.service.name=producer-svc","-Dotel.traces.exporter=otlp","-Dotel.metrics.exporter=otlp","-Dotel.logs.exporter=otlp","-jar","/Users/x/oss/kafka-opentelemetry-instrumentation/kafka-app-otel/kafka-producer/target/kafka-producer-1.0-SNAPSHOT-jar-with-dependencies.jar"])
     -> process.executable.path: Str(/Users/x/Library/Java/JavaVirtualMachines/openjdk-22.0.1/Contents/Home/bin/java)
     -> process.pid: Int(99185)
     -> process.runtime.description: Str(Oracle Corporation OpenJDK 64-Bit Server VM 22.0.1+8-16)
     -> process.runtime.name: Str(OpenJDK Runtime Environment)
     -> process.runtime.version: Str(22.0.1+8-16)
     -> service.instance.id: Str(482b41e0-dff9-4564-bdc3-442b5d84ae2e)
     -> service.name: Str(producer-svc)
     -> telemetry.distro.name: Str(opentelemetry-java-instrumentation)
     -> telemetry.distro.version: Str(2.4.0)
     -> telemetry.sdk.language: Str(java)
     -> telemetry.sdk.name: Str(opentelemetry)
     -> telemetry.sdk.version: Str(1.38.0)
ScopeMetrics #0
ScopeMetrics SchemaURL:
InstrumentationScope io.opentelemetry.sdk.trace
Metric #0
Descriptor:
     -> Name: processedSpans
     -> Description: The number of spans processed by the BatchSpanProcessor. [dropped=true if they were dropped due to high throughput]
     -> Unit: 1
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
Data point attributes:
     -> dropped: Bool(false)
     -> processorType: Str(BatchSpanProcessor)
StartTimestamp: 2024-05-22 15:58:09.766344 +0000 UTC
Timestamp: 2024-05-22 15:58:15.802274 +0000 UTC
Value: 50
Metric #1
Descriptor:
     -> Name: queueSize
     -> Description: The number of items queued
     -> Unit: 1
     -> DataType: Gauge
NumberDataPoints #0
Data point attributes:
     -> processorType: Str(BatchSpanProcessor)
StartTimestamp: 2024-05-22 15:58:09.766344 +0000 UTC
Timestamp: 2024-05-22 15:58:15.802274 +0000 UTC
Value: 0
ScopeMetrics #1
ScopeMetrics SchemaURL:
InstrumentationScope io.opentelemetry.exporters.otlp-http
Metric #0
Descriptor:
     -> Name: otlp.exporter.exported
     -> Description:
     -> Unit:
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
Data point attributes:
     -> success: Bool(true)
     -> type: Str(span)
StartTimestamp: 2024-05-22 15:58:09.766344 +0000 UTC
Timestamp: 2024-05-22 15:58:15.802274 +0000 UTC
Value: 50
NumberDataPoints #1
Data point attributes:
     -> success: Bool(true)
     -> type: Str(log)
StartTimestamp: 2024-05-22 15:58:09.766344 +0000 UTC
Timestamp: 2024-05-22 15:58:15.802274 +0000 UTC
Value: 63
Metric #1
Descriptor:
     -> Name: otlp.exporter.seen
     -> Description:
     -> Unit:
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
Data point attributes:
     -> type: Str(span)
StartTimestamp: 2024-05-22 15:58:09.766344 +0000 UTC
Timestamp: 2024-05-22 15:58:15.802274 +0000 UTC
Value: 50
NumberDataPoints #1
Data point attributes:
     -> type: Str(log)
StartTimestamp: 2024-05-22 15:58:09.766344 +0000 UTC
Timestamp: 2024-05-22 15:58:15.802274 +0000 UTC
Value: 63
ScopeMetrics #2
ScopeMetrics SchemaURL:
InstrumentationScope io.opentelemetry.sdk.logs
Metric #0
Descriptor:
     -> Name: processedLogs
     -> Description: The number of logs processed by the BatchLogRecordProcessor. [dropped=true if they were dropped due to high throughput]
     -> Unit: 1
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
Data point attributes:
     -> dropped: Bool(false)
     -> processorType: Str(BatchLogRecordProcessor)
StartTimestamp: 2024-05-22 15:58:09.766344 +0000 UTC
Timestamp: 2024-05-22 15:58:15.802274 +0000 UTC
Value: 63
Metric #1
Descriptor:
     -> Name: queueSize
     -> Description: The number of items queued
     -> Unit: 1
     -> DataType: Gauge
NumberDataPoints #0
Data point attributes:
     -> processorType: Str(BatchLogRecordProcessor)
StartTimestamp: 2024-05-22 15:58:09.766344 +0000 UTC
Timestamp: 2024-05-22 15:58:15.802274 +0000 UTC
Value: 0
ScopeMetrics #3
ScopeMetrics SchemaURL:
InstrumentationScope io.opentelemetry.runtime-telemetry-java8 2.4.0-alpha
Metric #0
Descriptor:
     -> Name: jvm.gc.duration
     -> Description: Duration of JVM garbage collection actions.
     -> Unit: s
     -> DataType: Histogram
     -> AggregationTemporality: Cumulative
HistogramDataPoints #0
Data point attributes:
     -> jvm.gc.action: Str(end of minor GC)
     -> jvm.gc.name: Str(G1 Young Generation)
StartTimestamp: 2024-05-22 15:58:09.766344 +0000 UTC
Timestamp: 2024-05-22 15:58:15.802274 +0000 UTC
Count: 4
Sum: 0.010000
Min: 0.002000
Max: 0.003000
ExplicitBounds #0: 0.010000
ExplicitBounds #1: 0.100000
ExplicitBounds #2: 1.000000
ExplicitBounds #3: 10.000000
Buckets #0, Count: 4
Buckets #1, Count: 0
Buckets #2, Count: 0
Buckets #3, Count: 0
Buckets #4, Count: 0
HistogramDataPoints #1
Data point attributes:
     -> jvm.gc.action: Str(end of concurrent GC pause)
     -> jvm.gc.name: Str(G1 Concurrent GC)
StartTimestamp: 2024-05-22 15:58:09.766344 +0000 UTC
Timestamp: 2024-05-22 15:58:15.802274 +0000 UTC
Count: 4
Sum: 0.004000
Min: 0.000000
Max: 0.003000
ExplicitBounds #0: 0.010000
ExplicitBounds #1: 0.100000
ExplicitBounds #2: 1.000000
ExplicitBounds #3: 10.000000
Buckets #0, Count: 4
Buckets #1, Count: 0
Buckets #2, Count: 0
Buckets #3, Count: 0
Buckets #4, Count: 0
	{"kind": "exporter", "data_type": "metrics", "name": "debug"}
2024-05-22T17:58:22.921+0200	info	MetricsExporter	{"kind": "exporter", "data_type": "metrics", "name": "debug", "resource metrics": 1, "metrics": 22, "data points": 35}
2024-05-22T17:58:22.921+0200	info	ResourceMetrics #0
Resource SchemaURL:
Resource attributes:
     -> service.name: Str(unknown_service:java)
     -> telemetry.sdk.language: Str(java)
     -> telemetry.sdk.name: Str(opentelemetry)
     -> telemetry.sdk.version: Str(1.35.0)
ScopeMetrics #0
ScopeMetrics SchemaURL:
InstrumentationScope io.opentelemetry.contrib.jmxmetrics 1.33.0-alpha
Metric #0
Descriptor:
     -> Name: kafka.leader.election.rate
     -> Description: leader election rate - increasing indicates broker failures
     -> Unit: {elections}
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0
Metric #1
Descriptor:
     -> Name: kafka.request.failed
     -> Description: The number of requests to the broker resulting in a failure
     -> Unit: {requests}
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
Data point attributes:
     -> type: Str(fetch)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0
NumberDataPoints #1
Data point attributes:
     -> type: Str(produce)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0
Metric #2
Descriptor:
     -> Name: kafka.purgatory.size
     -> Description: The number of requests waiting in purgatory
     -> Unit: {requests}
     -> DataType: Gauge
NumberDataPoints #0
Data point attributes:
     -> type: Str(fetch)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0
NumberDataPoints #1
Data point attributes:
     -> type: Str(produce)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0
Metric #3
Descriptor:
     -> Name: kafka.request.time.avg
     -> Description: The average time the broker has taken to service requests
     -> Unit: ms
     -> DataType: Gauge
NumberDataPoints #0
Data point attributes:
     -> type: Str(fetchfollower)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0.000000
NumberDataPoints #1
Data point attributes:
     -> type: Str(produce)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 43.000000
NumberDataPoints #2
Data point attributes:
     -> type: Str(fetchconsumer)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0.000000
Metric #4
Descriptor:
     -> Name: kafka.controller.active.count
     -> Description: controller is active on broker
     -> Unit: {controllers}
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 1
Metric #5
Descriptor:
     -> Name: kafka.request.time.total
     -> Description: The total time the broker has taken to service requests
     -> Unit: ms
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
Data point attributes:
     -> type: Str(fetchfollower)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0
NumberDataPoints #1
Data point attributes:
     -> type: Str(produce)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 1
NumberDataPoints #2
Data point attributes:
     -> type: Str(fetchconsumer)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0
Metric #6
Descriptor:
     -> Name: kafka.logs.flush.time.99p
     -> Description: log flush time - 99th percentile
     -> Unit: ms
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0.000000
Metric #7
Descriptor:
     -> Name: kafka.partition.under_replicated
     -> Description: The number of under replicated partitions
     -> Unit: {partitions}
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0
Metric #8
Descriptor:
     -> Name: kafka.logs.flush.time.median
     -> Description: log flush time - 50th percentile
     -> Unit: ms
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0.000000
Metric #9
Descriptor:
     -> Name: kafka.unclean.election.rate
     -> Description: unclean leader election rate - increasing indicates broker failures
     -> Unit: {elections}
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0
Metric #10
Descriptor:
     -> Name: kafka.network.io
     -> Description: The bytes received or sent by the broker
     -> Unit: by
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
Data point attributes:
     -> state: Str(out)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0
NumberDataPoints #1
Data point attributes:
     -> state: Str(in)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 4751
Metric #11
Descriptor:
     -> Name: kafka.partition.offline
     -> Description: The number of partitions offline
     -> Unit: {partitions}
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 1
Metric #12
Descriptor:
     -> Name: kafka.request.time.50p
     -> Description: The 50th percentile time the broker has taken to service requests
     -> Unit: ms
     -> DataType: Gauge
NumberDataPoints #0
Data point attributes:
     -> type: Str(fetchfollower)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0.000000
NumberDataPoints #1
Data point attributes:
     -> type: Str(produce)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 43.000000
NumberDataPoints #2
Data point attributes:
     -> type: Str(fetchconsumer)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0.000000
Metric #13
Descriptor:
     -> Name: kafka.logs.flush.time.count
     -> Description: log flush count
     -> Unit: ms
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0
Metric #14
Descriptor:
     -> Name: kafka.isr.operation.count
     -> Description: The number of in-sync replica shrink and expand operations
     -> Unit: {operations}
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
Data point attributes:
     -> operation: Str(shrink)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0
NumberDataPoints #1
Data point attributes:
     -> operation: Str(expand)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0
Metric #15
Descriptor:
     -> Name: kafka.request.queue
     -> Description: size of the request queue
     -> Unit: {requests}
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0
Metric #16
Descriptor:
     -> Name: kafka.request.time.99p
     -> Description: The 99th percentile time the broker has taken to service requests
     -> Unit: ms
     -> DataType: Gauge
NumberDataPoints #0
Data point attributes:
     -> type: Str(fetchfollower)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0.000000
NumberDataPoints #1
Data point attributes:
     -> type: Str(produce)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 43.000000
NumberDataPoints #2
Data point attributes:
     -> type: Str(fetchconsumer)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0.000000
Metric #17
Descriptor:
     -> Name: kafka.request.count
     -> Description: The number of requests received by the broker
     -> Unit: {requests}
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
Data point attributes:
     -> type: Str(fetch)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0
NumberDataPoints #1
Data point attributes:
     -> type: Str(produce)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 1
Metric #18
Descriptor:
     -> Name: kafka.message.count
     -> Description: The number of messages received by the broker
     -> Unit: {messages}
     -> DataType: Sum
     -> IsMonotonic: true
     -> AggregationTemporality: Cumulative
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 50
Metric #19
Descriptor:
     -> Name: kafka.partition.count
     -> Description: The number of partitions on the broker
     -> Unit: {partitions}
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 1
Metric #20
Descriptor:
     -> Name: kafka.max.lag
     -> Description: max lag in messages between follower and leader replicas
     -> Unit: {messages}
     -> DataType: Gauge
NumberDataPoints #0
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0
ScopeMetrics #1
ScopeMetrics SchemaURL:
InstrumentationScope io.opentelemetry.sdk.logs
Metric #0
Descriptor:
     -> Name: queueSize
     -> Description: The number of items queued
     -> Unit: 1
     -> DataType: Gauge
NumberDataPoints #0
Data point attributes:
     -> processorType: Str(BatchLogRecordProcessor)
StartTimestamp: 2024-05-22 15:57:52.893 +0000 UTC
Timestamp: 2024-05-22 15:58:22.899 +0000 UTC
Value: 0